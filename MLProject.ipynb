{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Model - Deep And Wide Neural Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df):\n",
    "    y = df[\"cancer_type\"]\n",
    "    label_encoder = LabelEncoder();\n",
    "    y  = label_encoder.fit_transform(y)\n",
    "    y = pd.Series(y)\n",
    "    df = df.drop('cancer_type', axis = 1)\n",
    "\n",
    "    # label encoding for cellularity 40 nan values transformed to 0\n",
    "    mapping = {\n",
    "        'Low': 1,\n",
    "        'Moderate': 2,\n",
    "        'High': 3,\n",
    "    }\n",
    "    df['cellularity'] = df['cellularity'].str.strip()\n",
    "    df[\"cellularity\"] = df[\"cellularity\"].map(mapping)\n",
    "    df[\"cellularity\"] = df[\"cellularity\"].fillna(0)\n",
    "\n",
    "\n",
    "    # dropping patient_id (irrelevant info)\n",
    "    df = df.drop('patient_id', axis=1)\n",
    "\n",
    "    #label encoding pam50_+_claudin-low_subtype\n",
    "    df['pam50_+_claudin-low_subtype'] =label_encoder.fit_transform( df['pam50_+_claudin-low_subtype'])\n",
    "\n",
    "    df['er_status'] =label_encoder.fit_transform( df['er_status'])\n",
    "\n",
    "    df['er_status_measured_by_ihc'] = label_encoder.fit_transform(df['er_status_measured_by_ihc'])\n",
    "\n",
    "    df['her2_status'] = label_encoder.fit_transform(df['her2_status'])\n",
    "\n",
    "    her2_mapping={\n",
    "    'LOSS' : 0,\n",
    "    'NEUTRAL' : 1,\n",
    "    'GAIN' : 3,\n",
    "    'UNDEF' : 1\n",
    "    }\n",
    "\n",
    "    df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].str.strip()\n",
    "    df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].map(her2_mapping)\n",
    "\n",
    "    df['inferred_menopausal_state'] = label_encoder.fit_transform(df['inferred_menopausal_state'])\n",
    "\n",
    "    map_laterality = {\n",
    "    'Right':1,\n",
    "    'Left':-1,\n",
    "    }\n",
    "    df['primary_tumor_laterality'] = df['primary_tumor_laterality'].str.strip()\n",
    "    df['primary_tumor_laterality'] = df['primary_tumor_laterality'].map(map_laterality)\n",
    "    df['primary_tumor_laterality'] = df['primary_tumor_laterality'].fillna(0)\n",
    "\n",
    "    df['pr_status'] = label_encoder.fit_transform(df['pr_status'])\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['3-gene_classifier_subtype'])\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['death_from_cancer'])\n",
    "\n",
    "    tumor_mean = df['tumor_size'].mean()\n",
    "    df[\"tumor_size\"] = df[\"tumor_size\"].fillna(tumor_mean)\n",
    "\n",
    "    mutation_mean = df['mutation_count'].mean()\n",
    "    df['mutation_count'] = df['mutation_count'].fillna(mutation_mean)\n",
    "\n",
    "    df['neoplasm_histologic_grade'] = df['neoplasm_histologic_grade'].fillna(3)\n",
    "\n",
    "    majority_value = df['tumor_stage'].mode()[0]\n",
    "    df['tumor_stage'].fillna(majority_value, inplace=True)\n",
    "    df['tumor_stage']=label_encoder.fit_transform(df['tumor_stage'])\n",
    "\n",
    "    label_encoders = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            # Create a label encoder for each categorical column\n",
    "            le = LabelEncoder()\n",
    "\n",
    "            # Fit the label encoder and transform the data\n",
    "            df[column] = le.fit_transform(df[column].astype(str))\n",
    "\n",
    "            # Store the label encoder in a dictionary in case you need to reverse the encoding or use it later\n",
    "            label_encoders[column] = le\n",
    "    \n",
    "    last_seven = df.iloc[:, -7:]\n",
    "    part_before = df.iloc[:, :2]  # Columns up to the 19th (0-based index, so it includes columns 0-18)\n",
    "    part_after = df.iloc[:, 2:]\n",
    "    df = pd.concat([part_before, last_seven, part_after], axis=1)\n",
    "    df = df.iloc[:, :-7]\n",
    "\n",
    "\n",
    "    \n",
    "    return df,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "X, y = data_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_and_wide(input, index_tumor_stage ):\n",
    "  input = tf.keras.layers.Input(shape = (input.shape[1],))\n",
    "  \n",
    "  input_d = input[:, :index_tumor_stage + 1]\n",
    "  input_w = input[:, index_tumor_stage + 1:]\n",
    "  d = tf.keras.layers.Dense(2048,activation='leaky_relu')(input_d)\n",
    "  d=  tf.keras.layers.Dense(1024, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(512, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(256, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(256, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(128, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(64, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(32, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(16, activation = 'relu')(d)\n",
    "  d=  tf.keras.layers.Dense(8, activation = 'relu')(d)\n",
    "\n",
    "  w = tf.keras.layers.Dense(256,activation = 'relu')(input_w)\n",
    "  w = tf.keras.layers.Dense(128 , activation = 'relu')(w)\n",
    "  w = tf.keras.layers.Dense(8 , activation = 'relu')(w)\n",
    "\n",
    "  combined = tf.keras.layers.concatenate([w, d])\n",
    "  combined = tf.keras.layers.Dense(10, activation = 'relu')(combined)\n",
    "  combined = tf.keras.layers.Dense(4, activation = 'softmax')(combined)\n",
    "\n",
    "  model_deep_wide = tf.keras.Model(inputs = input, outputs = combined)\n",
    "\n",
    "  model_deep_wide.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return model_deep_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.DataFrame(X)\n",
    "excel.to_csv('clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier = X.columns.get_loc('tumor_stage')\n",
    "d_w_model = deep_and_wide(X_train, frontier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=d_w_model.fit(X_train,y_train,  epochs = 15,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = d_w_model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, predicted_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=[0, 1, 2, 3], yticklabels=[0, 1, 2, 3])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evl=d_w_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Model - Hierarchical XGBoost Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import TrainingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 - Data Preprocessing \n",
    "# Load the data\n",
    "data = pd.read_csv('/content/data.csv', low_memory=False)\n",
    "\n",
    "# Function to drop columns that have the same value across all rows\n",
    "def remove_infrequent_categories(data, threshold=0.05):\n",
    "    filtered_data = data.copy()\n",
    "    categorical_columns = filtered_data.select_dtypes(include='object').columns\n",
    "    categorical_columns = [column for column in categorical_columns if column.endswith('_mut')]\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        filtered_data = filtered_data.loc[filtered_data[column].isin(filtered_data[column].value_counts().index[filtered_data[column].value_counts()/len(filtered_data) > threshold])]\n",
    "    return filtered_data\n",
    "\n",
    "def drop_single_class_columns(df):\n",
    "    unique_value_counts = df.nunique()\n",
    "    single_value_columns = unique_value_counts[unique_value_counts == 1].index\n",
    "    return df.drop(columns=single_value_columns)\n",
    "\n",
    "# Function to one-hot encode specified categorical columns\n",
    "def one_hot_encode_columns(df, columns, encoder, isTrain):\n",
    "    if isTrain:\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        encoded_data = encoder.fit_transform(df[columns])\n",
    "    else:\n",
    "        encoded_data = encoder.transform(df[columns])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns))\n",
    "    return df.drop(columns, axis=1).join(encoded_df), encoder\n",
    "\n",
    "# Main preprocessing function\n",
    "def data_preprocess(df, encoder=None, isTrain=True):\n",
    "    df = drop_single_class_columns(df)\n",
    "\n",
    "    # Fill missing values for numerical columns\n",
    "    numerical_columns = ['neoplasm_histologic_grade', 'mutation_count', 'tumor_size', 'tumor_stage']\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "\n",
    "    # Identify and fill missing values for all other numerical columns just in case\n",
    "    other_numerical_columns = df.select_dtypes(include=[np.number]).columns.difference(numerical_columns)\n",
    "    df[other_numerical_columns] = df[other_numerical_columns].fillna(df[other_numerical_columns].median())\n",
    "\n",
    "    # Handle missing values and encode categorical variables\n",
    "    categorical_columns = ['pr_status', 'pam50_+_claudin-low_subtype', 'primary_tumor_laterality', 'inferred_menopausal_state', 'her2_status', 'er_status', 'er_status_measured_by_ihc', '3-gene_classifier_subtype', 'death_from_cancer']\n",
    "    for column in categorical_columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    # Ensure all other categorical columns are also filled with the most frequent value\n",
    "    other_categorical_columns = df.select_dtypes(include=['object', 'category']).columns.difference(categorical_columns)\n",
    "    for column in other_categorical_columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    if isTrain:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_data = encoder.fit_transform(df[categorical_columns])\n",
    "    else:\n",
    "        encoded_data = encoder.transform(df[categorical_columns])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "    df = df.drop(categorical_columns, axis=1).join(encoded_df)\n",
    "\n",
    "    # Handling 'cellularity' with predefined mapping and filling NaNs\n",
    "    if 'cellularity' in df.columns:\n",
    "        df['cellularity'] = df['cellularity'].str.strip()  # Strip whitespace\n",
    "        mapping = {\n",
    "            'Low': 1,\n",
    "            'Moderate': 2,\n",
    "            'High': 3,\n",
    "        }\n",
    "        df['cellularity'] = df['cellularity'].map(mapping).fillna(2)  # Filling NaNs with 'Moderate' assumed as 2\n",
    "\n",
    "    # Handling 'her2_status_measured_by_snp6' with predefined mapping and dropping rows with 'UNDEF'\n",
    "    if 'her2_status_measured_by_snp6' in df.columns:\n",
    "        df = df[df['her2_status_measured_by_snp6'] != 'UNDEF']\n",
    "        df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].str.strip()\n",
    "        her2_mapping = {\n",
    "            'LOSS': -1,\n",
    "            'NEUTRAL': 0,\n",
    "            'GAIN': 1,\n",
    "        }\n",
    "        df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].map(her2_mapping)\n",
    "\n",
    "    # Label encoding for the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = df.pop(\"cancer_type\")\n",
    "    y_encoded = label_encoder.fit_transform(y) if isTrain else label_encoder.transform(y)\n",
    "    y_binary = (y_encoded == 0).astype(int)\n",
    "\n",
    "    return df, y_encoded, y_binary, label_encoder, encoder\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, y_binary, label_encoder, encoder = data_preprocess(data.copy(), isTrain=True)\n",
    "\n",
    "# Split data into train, validate, test sets\n",
    "X_train, X_temp, y_train, y_temp, y_train_binary, y_temp_binary = train_test_split(X, y, y_binary, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 - Train binary classification model\n",
    "# Split the training data for binary classification\n",
    "X_train_binary, _, y_train_binary, _ = train_test_split(X_train, y_train_binary, test_size=0.2, random_state=42)\n",
    "X_test_binary, X_val_binary, y_test_binary, y_val_binary = train_test_split(X_temp, y_temp_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "# Apply Random Over Sampler to the training data\n",
    "ros = RandomOverSampler(random_state=23)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train_binary, y_train_binary)\n",
    "\n",
    "# Adjusting the scale_pos_weight parameter for XGBoost based on class distribution\n",
    "scale_pos_weight = sum(y_train_ros == 0) / sum(y_train_ros == 1)\n",
    "\n",
    "# Convert all columns of type 'object' to 'category'\n",
    "categorical_columns = X_train_ros.select_dtypes(include=['object']).columns\n",
    "X_train_ros[categorical_columns] = X_train_ros[categorical_columns].astype('category')\n",
    "X_test_binary[categorical_columns] = X_test_binary[categorical_columns].astype('category')\n",
    "X_val_binary[categorical_columns] = X_val_binary[categorical_columns].astype('category')\n",
    "\n",
    "# Create DMatrix for train and test sets\n",
    "dtrain_binary = xgb.DMatrix(X_train_ros, label=y_train_ros, enable_categorical=True)\n",
    "dtest_binary = xgb.DMatrix(X_test_binary, label=y_test_binary, enable_categorical=True)\n",
    "dval_binary = xgb.DMatrix(X_val_binary, label=y_val_binary, enable_categorical=True)\n",
    "\n",
    "# Specify parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'num_class': 1,\n",
    "    'device':'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1,\n",
    "    'alpha': 0.01,\n",
    "    'scale_pos_weight': scale_pos_weight,  # Applying class weight\n",
    "    'num_parallel_tree':5,\n",
    "}\n",
    "\n",
    "# Define the evaluation log storage callback, inheriting from TrainingCallback\n",
    "class EvaluationHistoryCallback(TrainingCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.evaluation_results = {}\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if not evals_log:\n",
    "            return False\n",
    "        for dataset_name, metric_dict in evals_log.items():\n",
    "            for metric_name, log in metric_dict.items():\n",
    "                full_metric_name = f\"{dataset_name}-{metric_name}\"\n",
    "                if full_metric_name not in self.evaluation_results:\n",
    "                    self.evaluation_results[full_metric_name] = []\n",
    "                self.evaluation_results[full_metric_name].append(log[-1])\n",
    "        return False  # Return False to continue training\n",
    "\n",
    "# Initialize callback\n",
    "eval_history = EvaluationHistoryCallback()\n",
    "\n",
    "# Train the model with evaluation set and custom callback\n",
    "eval_set = [(dtrain_binary, 'train'), (dval_binary, 'eval')]\n",
    "binary_model = xgb.train(params, dtrain_binary, num_boost_round=100, evals=eval_set, early_stopping_rounds=10, verbose_eval=True, callbacks=[eval_history])\n",
    "\n",
    "# Access the stored evaluation results\n",
    "results = eval_history.evaluation_results\n",
    "train_rmse = results.get('train-rmse', [])\n",
    "eval_rmse = results.get('eval-rmse', [])\n",
    "epochs = len(train_rmse)\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# Plot learning curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, train_rmse, label='Train RMSE')\n",
    "ax.plot(x_axis, eval_rmse, label='Validation RMSE')\n",
    "ax.legend()\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve for Binary Model')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_binary = binary_model.predict(dtest_binary)\n",
    "y_pred_binary = np.round(y_pred_binary)  # Convert probabilities to binary output\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Binary Model Accuracy:\", accuracy_score(y_test_binary, y_pred_binary))\n",
    "print(\"Binary Model Confusion Matrix:\\n\", confusion_matrix(y_test_binary, y_pred_binary))\n",
    "print(\"Binary Model Classification Report:\\n\", classification_report(y_test_binary, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 - Prepare and train multi-class model\n",
    "# Apply preprocessing and ensure the original target reflects correctly\n",
    "processed_data, y_encoded, y_binary, label_encoder, encoder = data_preprocess(data.copy(), isTrain=True)\n",
    "\n",
    "# Reset indices of processed_data to ensure alignment\n",
    "processed_data.reset_index(drop=True, inplace=True)\n",
    "y_encoded = pd.Series(y_encoded)\n",
    "\n",
    "# Confirming the filtered targets\n",
    "print(\"Unique values in y_encoded:\", np.unique(y_encoded))\n",
    "print(\"Classes found by LabelEncoder:\", label_encoder.classes_)\n",
    "\n",
    "# Assuming 'cellularity' should map directly for filtering but not as the target\n",
    "if 'cellularity' in processed_data.columns:\n",
    "    # Print unique values for diagnostics\n",
    "    print(\"Unique values in cellularity before filtering:\", processed_data['cellularity'].unique())\n",
    "\n",
    "    # Filter data based on 'cellularity' values and exclude class 0\n",
    "    filtered_data = processed_data[(processed_data['cellularity'].isin([1, 2, 3])) & (y_encoded != 0)]\n",
    "    \n",
    "    # Ensure y_encoded aligns with the filtered data\n",
    "    filtered_y_encoded = y_encoded[filtered_data.index].reset_index(drop=True)\n",
    "    \n",
    "     # Re-encode the target variable to adjust for the missing class\n",
    "    label_encoder = LabelEncoder()\n",
    "    filtered_y_encoded = label_encoder.fit_transform(filtered_y_encoded)\n",
    "    print(\"Re-encoded classes after filtering and exclusion:\", label_encoder.classes_)\n",
    "\n",
    "    # check the filtered results\n",
    "    print(\"Filtered Data Shape:\", filtered_data.shape)\n",
    "    print(\"Filtered Target Shape:\", filtered_y_encoded.shape)\n",
    "\n",
    "unique_classes_filtered = np.unique(filtered_y_encoded)\n",
    "print(\"Unique classes after filtering:\", unique_classes_filtered)\n",
    "\n",
    "# Proceed with train-test split and resampling\n",
    "X_train_multi, X_temp_multi, y_train_multi, y_temp_multi = train_test_split(filtered_data, filtered_y_encoded, test_size=0.2, random_state=42)\n",
    "X_test_multi, X_val_multi, y_test_multi, y_val_multi = train_test_split(X_temp_multi, y_temp_multi, test_size=0.5, random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "ros_multi = RandomOverSampler(random_state=42)\n",
    "X_train_multi_ros, y_train_multi_ros = ros_multi.fit_resample(X_train_multi, y_train_multi)\n",
    "\n",
    "# Recheck the unique values in y_train_multi_ros\n",
    "print(\"Unique y values in training data after resample:\", np.unique(y_train_multi_ros))\n",
    "\n",
    "# Adjusting the scale_pos_weight parameter for XGBoost based on class distribution\n",
    "scale_pos_weight_multi = {}\n",
    "for class_label in np.unique(y_train_multi_ros):\n",
    "    scale_pos_weight_multi[class_label] = sum(y_train_multi_ros == class_label) / sum(y_train_multi_ros != class_label)\n",
    "\n",
    "# Convert object columns to category type\n",
    "object_columns = X_train_multi_ros.select_dtypes(include=['object']).columns\n",
    "if not object_columns.empty:\n",
    "    X_train_multi_ros[object_columns] = X_train_multi_ros[object_columns].astype('category')\n",
    "    X_test_multi[object_columns] = X_test_multi[object_columns].astype('category')\n",
    "    X_val_multi[object_columns] = X_val_multi[object_columns].astype('category')\n",
    "\n",
    "print(np.unique(y_train_multi_ros))\n",
    "\n",
    "# Specify parameters\n",
    "params_multi = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': len(np.unique(filtered_y_encoded)),\n",
    "    'eval_metric': 'mlogloss',  # Changed from rmse to mlogloss for multi-class\n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 15,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1.5,\n",
    "    'alpha': 0.01,\n",
    "    'num_parallel_tree': 5\n",
    "}\n",
    "\n",
    "# Prepare DMatrix for training and validation\n",
    "dtrain_multi = xgb.DMatrix(X_train_multi_ros, label=y_train_multi_ros, enable_categorical=True)\n",
    "dval_multi = xgb.DMatrix(X_val_multi, label=y_val_multi, enable_categorical=True)\n",
    "dtest_multi = xgb.DMatrix(X_test_multi, label=y_test_multi, enable_categorical=True)\n",
    "\n",
    "# Check the sizes right before training\n",
    "print(\"Training size:\", dtrain_multi.num_row(), \"Labels:\", len(y_train_multi_ros))\n",
    "print(\"Validation size:\", dval_multi.num_row(), \"Labels:\", len(y_val_multi))\n",
    "\n",
    "# Initialize callback\n",
    "eval_history_multi = EvaluationHistoryCallback()\n",
    "\n",
    "# Train the model with evaluation set and custom callback\n",
    "eval_set_multi = [(dtrain_multi, 'train'), (dval_multi, 'eval')]\n",
    "multi_class_model = xgb.train(params_multi, dtrain_multi, num_boost_round=100, evals=eval_set_multi, early_stopping_rounds=10, verbose_eval=True, callbacks=[eval_history_multi])\n",
    "\n",
    "# Access the stored evaluation results\n",
    "results_multi = eval_history_multi.evaluation_results\n",
    "train_mlogloss = results_multi.get('train-mlogloss', [])\n",
    "eval_mlogloss = results_multi.get('eval-mlogloss', [])\n",
    "epochs_multi = len(train_mlogloss)\n",
    "x_axis_multi = range(0, epochs_multi)\n",
    "\n",
    "# Plot learning curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis_multi, train_mlogloss, label='Train MLogLoss')\n",
    "ax.plot(x_axis_multi, eval_mlogloss, label='Validation MLogLoss')\n",
    "ax.legend()\n",
    "plt.ylabel('MLogLoss')\n",
    "plt.title('Learning Curve for Multi-Class Model')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_multi = multi_class_model.predict(dtest_multi)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Multi-Class Model Accuracy:\", accuracy_score(y_test_multi, y_pred_multi))\n",
    "print(\"Multi-Class Model Confusion Matrix:\\n\", confusion_matrix(y_test_multi, y_pred_multi))\n",
    "print(\"Multi-Class Model Classification Report:\\n\", classification_report(y_test_multi, y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 - Tuning the first Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Possible ranges of hyperparameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Function to perform manual cross-validation\n",
    "def manual_cv(params, n_splits=3):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train_ros):\n",
    "        X_train_kf, X_test_kf = X_train_ros.iloc[train_index], X_train_ros.iloc[test_index]\n",
    "        y_train_kf, y_test_kf = y_train_ros[train_index], y_train_ros[test_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_kf, label=y_train_kf, enable_categorical=True)\n",
    "        dtest = xgb.DMatrix(X_test_kf, label=y_test_kf, enable_categorical=True)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=params['num_boost_round'])\n",
    "        y_pred = model.predict(dtest)\n",
    "        y_pred_binary = np.round(y_pred)\n",
    "        auc = roc_auc_score(y_test_kf, y_pred_binary)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Iterate over combinations of parameters\n",
    "from itertools import product\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "for combination in product(*param_grid.values()):\n",
    "    params = dict(zip(param_grid.keys(), combination))\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['scale_pos_weight'] = scale_pos_weight  # Keep the scale_pos_weight from your original setup\n",
    "    params['tree_method'] = 'gpu_hist'  # Ensure to use GPU if available\n",
    "\n",
    "    current_score = manual_cv(params)\n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "        best_params = params\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "final_dtrain = xgb.DMatrix(X_train_ros, label=y_train_ros, enable_categorical=True)\n",
    "final_model = xgb.train(best_params, final_dtrain, num_boost_round=best_params['num_boost_round'])\n",
    "\n",
    "# Evaluate the final model\n",
    "final_dtest = xgb.DMatrix(X_test_binary, label=y_test_binary)\n",
    "y_pred_final = final_model.predict(final_dtest)\n",
    "y_pred_final = np.round(y_pred_final)  # Convert probabilities to binary output\n",
    "\n",
    "print(\"Final Model Accuracy:\", accuracy_score(y_test_binary, y_pred_final))\n",
    "print(\"Final Model Confusion Matrix:\\n\", confusion_matrix(y_test_binary, y_pred_final))\n",
    "print(\"Final Model Classification Report:\\n\", classification_report(y_test_binary, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Model - Another Neural Network Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optuna\n",
    "%pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def data_preprocess(df):\n",
    "    # One-hot encoding for the 'cancer_type' target variable\n",
    "    y = pd.get_dummies(df['cancer_type'], prefix='cancer_type')\n",
    "    df = df.drop('cancer_type', axis=1)\n",
    "\n",
    "    # Assume all other categorical data has been appropriately handled and included in df\n",
    "\n",
    "    # Scaling numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Convert the one-hot encoded y DataFrame to a single column with integer labels\n",
    "    y = np.argmax(y.values, axis=1)\n",
    "\n",
    "    return df, y\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Preprocess data\n",
    "X, y = data_preprocess(df)\n",
    "\n",
    "# Calculate class weights (for one-dimensional integer-encoded y)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np  \n",
    "\n",
    "# Assuming you have your training data loaded into X_train and y_train\n",
    "# Number of unique classes\n",
    "num_classes = len(np.unique(y_train))  # Adjusted to find the number of unique classes directly\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.02), input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.13),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "    Dropout(0.13),\n",
    "    Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.02))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Make sure this matches your label format\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary to check the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for any remaining NaN values in the dataset\n",
    "if np.any(np.isnan(X_train)) or np.any(np.isnan(y_train)):\n",
    "    X_train = np.nan_to_num(X_train)\n",
    "    y_train = np.nan_to_num(y_train)\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weights_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "try:\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "    # Predict the test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "except ValueError as e:\n",
    "    print(\"ValueError during model evaluation:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_model(trial):\n",
    "    # Hyperparameters to tune\n",
    "    optimizer_options = ['adam', 'sgd']\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    neurons = trial.suggest_categorical('neurons', [32, 64, 128])\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 5)  # Number of hidden layers to add\n",
    "\n",
    "    model = Sequential([Input(shape=(X_train.shape[1],))])\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model with dynamic optimizer\n",
    "    optimizer = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=lr)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        verbose=0,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Output the best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Function to create the model, required for KerasClassifier\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1],)),  # Using Input to specify input shape\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "        Dropout(0.13),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.02)),\n",
    "        Dropout(0.13),\n",
    "        Dense(len(np.unique(y_train)), activation='softmax', kernel_regularizer=l2(0.02))\n",
    "    ])\n",
    "    model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model with KerasClassifier\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set to report its accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "NN_score = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score of my best Neural Network: ', NN_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model - XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING NECESSARY LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import itertools as itr\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns=['pr_status',  'pam50_+_claudin-low_subtype', 'primary_tumor_laterality',   'inferred_menopausal_state',   'her2_status',   'er_status',   'er_status_measured_by_ihc',   '3-gene_classifier_subtype', 'death_from_cancer']\n",
    "\n",
    "def one_hot_encode_columns(df, columns,isTrain):\n",
    "  enc=OneHotEncoder(sparse_output=False)\n",
    "  encoded_data=enc.fit_transform(df[columns])\n",
    "  encoded_df = pd.DataFrame(encoded_data, columns=enc.get_feature_names_out(columns))\n",
    "  df_encoded = pd.concat([df.reset_index(drop=True),encoded_df.reset_index(drop=True)],axis=1)\n",
    "  return  df_encoded.drop(columns,axis=1)\n",
    "\n",
    "def data_preprocess(df,isTrain):\n",
    "\n",
    "  df = df.drop('patient_id', axis=1)\n",
    "  df=df.drop('cohort',axis=1)\n",
    "\n",
    "  numerical_columns = ['neoplasm_histologic_grade', 'mutation_count', 'tumor_size', 'tumor_stage']\n",
    "  df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "\n",
    "  categorical_columns = ['er_status_measured_by_ihc', 'primary_tumor_laterality', '3-gene_classifier_subtype', 'death_from_cancer']\n",
    "  for column in categorical_columns:\n",
    "    most_frequent_class = df[column].mode()[0]\n",
    "    df[column] = df[column].fillna(most_frequent_class)\n",
    "\n",
    "  # label encoding for cellularity 40 nan values transformed to 0\n",
    "  mapping = {\n",
    "      'Low': 1,\n",
    "      'Moderate': 2,\n",
    "      'High': 3,\n",
    "  }\n",
    "\n",
    "  df['cellularity'] = df['cellularity'].str.strip()\n",
    "  df[\"cellularity\"] = df[\"cellularity\"].map(mapping)\n",
    "  df[\"cellularity\"] = df[\"cellularity\"].fillna(3)\n",
    "\n",
    "  her2_mapping={\n",
    "  'LOSS' : -1,\n",
    "  'NEUTRAL' : 0,\n",
    "  'GAIN' : 1,\n",
    "  }\n",
    "\n",
    "  df=df[df.her2_status_measured_by_snp6 != 'UNDEF']\n",
    "  df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].str.strip()\n",
    "  df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].map(her2_mapping)\n",
    "\n",
    "  label_encoders = {}\n",
    "  y = df[\"cancer_type\"]\n",
    "  df = df.drop('cancer_type', axis = 1) \n",
    "  label_encoder = LabelEncoder()\n",
    "  y  = label_encoder.fit_transform(y)\n",
    "  y = pd.Series(y,name='cancer_type')\n",
    "\n",
    "  df=one_hot_encode_columns(df,cat_columns,isTrain=isTrain)\n",
    "  \n",
    "  for column in df.columns:\n",
    "      if df[column].dtype == 'object' and column not in ['pr_status',  'pam50_+_claudin-low_subtype', 'primary_tumor_laterality',   'inferred_menopausal_state',   'her2_status',   'er_status',   'er_status_measured_by_ihc',   '3-gene_classifier_subtype', 'death_from_cancer']:\n",
    "          \n",
    "          le = LabelEncoder()\n",
    "\n",
    "          df[column] = le.fit_transform(df[column].astype(str))\n",
    "\n",
    "          label_encoders[column] = le\n",
    "  \n",
    "  last_seven = df.iloc[:, -7:]\n",
    "  part_before = df.iloc[:, :2] \n",
    "  part_after = df.iloc[:, 2:]\n",
    "  df = pd.concat([part_before, last_seven, part_after], axis=1)\n",
    "  df = df.iloc[:, :-7]\n",
    "  \n",
    "  return df,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y=data_preprocess(data.copy(),isTrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_samples(X, y, desired_samples):\n",
    "    def get_class_indices(y):\n",
    "        class_indices = {}\n",
    "        for i, label in enumerate(np.unique(y)):\n",
    "            class_indices[label] = np.where(y == label)[0]\n",
    "        return class_indices\n",
    "    \n",
    "    class_indices = get_class_indices(y)\n",
    "    \n",
    "    all_indices = np.arange(len(X))\n",
    "    selected = []\n",
    "    X_train_selected = pd.DataFrame()\n",
    "    y_train_selected = []\n",
    "\n",
    "    X_test_selected = pd.DataFrame()\n",
    "    y_test_selected = []\n",
    "\n",
    "    for class_label, num_samples in desired_samples.items():\n",
    "        indices = class_indices[class_label]\n",
    "        selected_indices = np.random.choice(indices, size=num_samples, replace=False)\n",
    "        selected.extend(selected_indices.tolist())\n",
    "        X_train_selected = pd.concat([X_train_selected, X.loc[selected_indices]])\n",
    "        y_train_selected.extend(y[selected_indices])\n",
    "\n",
    "    test_indices = np.setdiff1d(all_indices, sorted(selected))\n",
    "    X_test_selected = pd.concat([X_test_selected, X.loc[test_indices]])\n",
    "    y_test_selected.extend(y[test_indices])\n",
    "\n",
    "    return X_train_selected, pd.Series(y_train_selected), X_test_selected, pd.Series(y_test_selected)\n",
    "\n",
    "\n",
    "\n",
    "desired_samples = {0: 700, 1: 100,2:12,3:150}\n",
    "X_train, y_train,X_test, y_test=split_data_by_samples(X,y,desired_samples=desired_samples)\n",
    "X_validate, X_test, y_validate, y_test =train_test_split(X_test,y_test,shuffle=True,test_size=0.5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERSAMPLING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "ROS=imblearn.over_sampling.RandomOverSampler(random_state=0)\n",
    "X_train, y_train=ROS.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgb_classifier = XGBClassifier(num_class=4,n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = xgb_classifier.predict(X_validate)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_validate, y_pred)\n",
    "\n",
    "print(\"acc Score on Validation Set:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEARNING CURVE PLOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.callback import TrainingCallback\n",
    "class EvaluationHistoryCallback(TrainingCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.evaluation_results = {}\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if not evals_log:\n",
    "            return False\n",
    "        for dataset_name, metric_dict in evals_log.items():\n",
    "            for metric_name, log in metric_dict.items():\n",
    "                full_metric_name = f\"{dataset_name}-{metric_name}\"\n",
    "                if full_metric_name not in self.evaluation_results:\n",
    "                    self.evaluation_results[full_metric_name] = []\n",
    "                self.evaluation_results[full_metric_name].append(log[-1])\n",
    "        return False  # Return False to continue training\n",
    "\n",
    "# Initialize callback\n",
    "eval_history_multi = EvaluationHistoryCallback()\n",
    "params_multi = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 4,\n",
    "    'eval_metric': 'mlogloss',  \n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 15,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1.5,\n",
    "    'alpha': 0.01,\n",
    "    'num_parallel_tree': 5\n",
    "}\n",
    "\n",
    "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
    "dval=xgb.DMatrix(X_validate,label=y_validate)\n",
    "eval_set_multi = [(dtrain, 'train'), (dval, 'eval')]\n",
    "multi_class_model = xgb.train(params_multi, dtrain, num_boost_round=100, evals=eval_set_multi, early_stopping_rounds=10, verbose_eval=True, callbacks=[eval_history_multi])\n",
    "\n",
    "# Access the stored evaluation results\n",
    "results_multi = eval_history_multi.evaluation_results\n",
    "train_mlogloss = results_multi.get('train-mlogloss', [])\n",
    "eval_mlogloss = results_multi.get('eval-mlogloss', [])\n",
    "epochs_multi = len(train_mlogloss)\n",
    "x_axis_multi = range(0, epochs_multi)\n",
    "\n",
    "# Plot learning curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis_multi, train_mlogloss, label='Train MLogLoss')\n",
    "ax.plot(x_axis_multi, eval_mlogloss, label='Validation MLogLoss')\n",
    "ax.legend()\n",
    "plt.ylabel('MLogLoss')\n",
    "plt.title('Learning Curve for Multi-Class Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "#predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = f1_score(y_validate, y_pred, average='macro')\n",
    "\n",
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_validate, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_validate), yticklabels=np.unique(y_validate))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "hyper_params={\n",
    "    'objective':['multi:softmax'],\n",
    "    'eval_metric':['logloss','mse','mae'],\n",
    "\t'eta':[0.01,0.1,0.3,0.9],\n",
    "\t'gamma':[0.05,0.1,0.2],\n",
    "\t'max_depth':[3,6,12],\n",
    "\t'n_estimators':[100,200,300],\n",
    "    'num_parallel_tree':[3,10,30],\n",
    "\t'subsample':[0.3,0.5,0.8],\n",
    "\t'sampling_method':['uniform','gradient_based'],\n",
    "\t'lambda':[0.1, 0.5 , 1, 5, 7,10],\n",
    "    'alpha':[0.01,0.05,0.1,0.3,1],\n",
    "\t'tree_method':['hist','approx'],\n",
    "\t'grow_policy':['depthwise','lossguide'],\n",
    "}\n",
    "allParams=sorted(hyper_params)\n",
    "\n",
    "combinations=list(itr.product(*(hyper_params[param] for param in allParams)))\n",
    "\n",
    "\n",
    "def grid_search_xgb(parameters, X_train, y_train, cv=2):\n",
    "  \"\"\"\n",
    "  Performs grid search with KFold cross-validation for XGBoost with DMatrix.\n",
    "\n",
    "  Args:\n",
    "      params: Dictionary of hyperparameter grids to search over.\n",
    "      x_train: DMatrix containing training data features.\n",
    "      y_train: Training data target labels.\n",
    "      cv: Number of folds for KFold cross-validation (default=5).\n",
    "      scoring: Evaluation metric for scoring models (default='neg_mean_squared_error').\n",
    "      eval_metric: Evaluation metric reported during training (default='rmse').\n",
    "\n",
    "  Returns:\n",
    "      best_params: Dictionary containing the best hyperparameters found.\n",
    "      best_model: The XGBoost model with the best hyperparameters.\n",
    "      cv_results: Dictionary containing cross-validation results for each parameter combination.\n",
    "  \"\"\"\n",
    "\n",
    "  best_params = None\n",
    "  f1_scores=[float('-inf'),float('-inf'),float('-inf')]\n",
    "\n",
    "  cv_results = {}\n",
    "\n",
    "\n",
    "  kfold = KFold(n_splits=cv, shuffle=True)\n",
    "\n",
    "  for combination in parameters:\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train):\n",
    "      x_train_fold = X_train.iloc[train_idx]\n",
    "      x_val_fold = X_train.iloc[val_idx]\n",
    "      y_train_fold = y_train[train_idx]\n",
    "      y_val_fold = y_train[val_idx]\n",
    "\n",
    "\n",
    "      train_dmatrix = xgb.DMatrix(x_train_fold, label=y_train_fold)\n",
    "      val_dmatrix = xgb.DMatrix(x_val_fold, label=y_val_fold)\n",
    "\n",
    "      params = {\n",
    "          'num_class': 4,\n",
    "          'alpha': combination[0],\n",
    "          'device': 'cuda',\n",
    "          'eta': combination[1],\n",
    "          'eval_metric': 'logloss',\n",
    "          'gamma': combination[3],\n",
    "          'grow_policy': combination[4],\n",
    "          'lambda': combination[5],\n",
    "          'max_depth': combination[6],\n",
    "          'num_parallel_tree': combination[8],\n",
    "          'objective': 'multi:softmax',\n",
    "          'sampling_method': combination[10],\n",
    "          'subsample': combination[11],\n",
    "          'tree_method': combination[12],\n",
    "      }\n",
    "\n",
    "\n",
    "      model = xgb.train(params, train_dmatrix,\n",
    "                  num_boost_round=200)\n",
    "\n",
    "      predictions=model.predict(val_dmatrix)\n",
    "      report=classification_report(y_val_fold,predictions,output_dict=True)\n",
    "      print(f'{report} for parameters {params}')\n",
    "      flags=[False,False,False]\n",
    "      for key, value in report.items():\n",
    "        if key==1 and f1_scores[key]<value:\n",
    "          f1_scores[key]=value\n",
    "          flags[key-1]=True\n",
    "        elif key==2 and f1_scores[key]<value:\n",
    "          f1_scores[key]=value\n",
    "          flags[key-1]=True\n",
    "        elif key==3 and f1_scores[key]<value:\n",
    "          f1_scores[key]=value\n",
    "          flags[key-1]=value\n",
    "\n",
    "      if flags[0]==True and flags[1]==True and flags[2]==True:\n",
    "        best_scores = f1_scores\n",
    "        best_params = combination\n",
    "        best_model = model\n",
    "\n",
    "      if combination not in cv_results:\n",
    "        cv_results[combination] = []\n",
    "      cv_results[combination].append(f1_scores)\n",
    "\n",
    "  return best_params, best_model, cv_results\n",
    "\n",
    "\n",
    "best_params, best_model, cv_results = grid_search_xgb(combinations, X_train, y_train)\n",
    "\n",
    "# Access results\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Model Score: {cv_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
