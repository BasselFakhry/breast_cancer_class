{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('/content/data.csv', low_memory=False)\n",
    "\n",
    "# Function to drop columns that have the same value across all rows\n",
    "def remove_infrequent_categories(data, threshold=0.05):\n",
    "    filtered_data = data.copy()\n",
    "    categorical_columns = filtered_data.select_dtypes(include='object').columns\n",
    "    categorical_columns = [column for column in categorical_columns if column.endswith('_mut')]\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        filtered_data = filtered_data.loc[filtered_data[column].isin(filtered_data[column].value_counts().index[filtered_data[column].value_counts()/len(filtered_data) > threshold])]\n",
    "    return filtered_data\n",
    "\n",
    "def drop_single_class_columns(df):\n",
    "    unique_value_counts = df.nunique()\n",
    "    single_value_columns = unique_value_counts[unique_value_counts == 1].index\n",
    "    return df.drop(columns=single_value_columns)\n",
    "\n",
    "# Function to one-hot encode specified categorical columns\n",
    "def one_hot_encode_columns(df, columns, encoder, isTrain):\n",
    "    if isTrain:\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        encoded_data = encoder.fit_transform(df[columns])\n",
    "    else:\n",
    "        encoded_data = encoder.transform(df[columns])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns))\n",
    "    return df.drop(columns, axis=1).join(encoded_df), encoder\n",
    "\n",
    "# Main preprocessing function\n",
    "def data_preprocess(df, encoder=None, isTrain=True):\n",
    "\n",
    "    df = drop_single_class_columns(df)\n",
    "\n",
    "    # Fill missing values for numerical columns\n",
    "    numerical_columns = ['neoplasm_histologic_grade', 'mutation_count', 'tumor_size', 'tumor_stage']\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "\n",
    "    # Identify and fill missing values for all other numerical columns just in case\n",
    "    other_numerical_columns = df.select_dtypes(include=[np.number]).columns.difference(numerical_columns)\n",
    "    df[other_numerical_columns] = df[other_numerical_columns].fillna(df[other_numerical_columns].median())\n",
    "\n",
    "    # Handle missing values and encode categorical variables\n",
    "    categorical_columns = ['pr_status', 'pam50_+_claudin-low_subtype', 'primary_tumor_laterality', 'inferred_menopausal_state', 'her2_status', 'er_status', 'er_status_measured_by_ihc', '3-gene_classifier_subtype', 'death_from_cancer']\n",
    "    for column in categorical_columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    # Ensure all other categorical columns are also filled with the most frequent value\n",
    "    other_categorical_columns = df.select_dtypes(include=['object', 'category']).columns.difference(categorical_columns)\n",
    "    for column in other_categorical_columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    if isTrain:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_data = encoder.fit_transform(df[categorical_columns])\n",
    "    else:\n",
    "        encoded_data = encoder.transform(df[categorical_columns])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "    df = df.drop(categorical_columns, axis=1).join(encoded_df)\n",
    "\n",
    "    # Handling 'cellularity' with predefined mapping and filling NaNs\n",
    "    if 'cellularity' in df.columns:\n",
    "        df['cellularity'] = df['cellularity'].str.strip()  # Strip whitespace\n",
    "        mapping = {\n",
    "            'Low': 1,\n",
    "            'Moderate': 2,\n",
    "            'High': 3,\n",
    "        }\n",
    "        df['cellularity'] = df['cellularity'].map(mapping).fillna(2)  # Filling NaNs with 'Moderate' assumed as 2\n",
    "\n",
    "    # Handling 'her2_status_measured_by_snp6' with predefined mapping and dropping rows with 'UNDEF'\n",
    "    if 'her2_status_measured_by_snp6' in df.columns:\n",
    "        df = df[df['her2_status_measured_by_snp6'] != 'UNDEF']\n",
    "        df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].str.strip()\n",
    "        her2_mapping = {\n",
    "            'LOSS': -1,\n",
    "            'NEUTRAL': 0,\n",
    "            'GAIN': 1,\n",
    "        }\n",
    "        df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].map(her2_mapping)\n",
    "\n",
    "    # Label encoding for the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = df.pop(\"cancer_type\")\n",
    "    y_encoded = label_encoder.fit_transform(y) if isTrain else label_encoder.transform(y)\n",
    "    y_binary = (y_encoded == 0).astype(int)\n",
    "\n",
    "    return df, y_encoded, y_binary, label_encoder, encoder\n",
    "\n",
    "# Preprocess data\n",
    "X, y, y_binary, label_encoder, encoder = data_preprocess(data.copy(), isTrain=True)\n",
    "\n",
    "# Split data into train, validate, test sets\n",
    "X_train, X_temp, y_train, y_temp, y_train_binary, y_temp_binary = train_test_split(X, y, y_binary, test_size=0.4, random_state=42)\n",
    "\n",
    "# Train binary classification model\n",
    "# Split the training data for binary classification\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_train, y_train_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Random Over Sampler to the training data\n",
    "ros = RandomOverSampler(random_state=23)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train_binary, y_train_binary)\n",
    "\n",
    "# Adjusting the scale_pos_weight parameter for XGBoost based on class distribution\n",
    "scale_pos_weight = sum(y_train_ros == 0) / sum(y_train_ros == 1)\n",
    "\n",
    "# Convert all columns of type 'object' to 'category'\n",
    "categorical_columns = X_train_ros.select_dtypes(include=['object']).columns\n",
    "X_train_ros[categorical_columns] = X_train_ros[categorical_columns].astype('category')\n",
    "X_test_binary[categorical_columns] = X_test_binary[categorical_columns].astype('category')\n",
    "\n",
    "# Create DMatrix for train and test sets\n",
    "dtrain_binary = xgb.DMatrix(X_train_ros, label=y_train_ros, enable_categorical=True)\n",
    "dtest_binary = xgb.DMatrix(X_test_binary, label=y_test_binary, enable_categorical=True)\n",
    "\n",
    "# Specify parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'num_class': 1,\n",
    "    'device':'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1,\n",
    "    'alpha': 0.01,\n",
    "    'scale_pos_weight': scale_pos_weight,  # Applying class weight\n",
    "    'num_parallel_tree':5,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "binary_model = xgb.train(params, dtrain_binary, num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_binary = binary_model.predict(dtest_binary)\n",
    "y_pred_binary = np.round(y_pred_binary)  # Convert probabilities to binary output\n",
    "\n",
    "# Evaluate model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "print(\"Binary Model Accuracy:\", accuracy_score(y_test_binary, y_pred_binary))\n",
    "print(\"Binary Model Confusion Matrix:\\n\", confusion_matrix(y_test_binary, y_pred_binary))\n",
    "print(\"Binary Model Classification Report:\\n\", classification_report(y_test_binary, y_pred_binary))\n",
    "\n",
    "# Prepare and train multi-class model\n",
    "# Apply preprocessing and ensure the target reflects 'cellularity' correctly\n",
    "processed_data, y_encoded, y_binary, label_encoder, encoder = data_preprocess(data.copy(), isTrain=True)\n",
    "\n",
    "# Reset indices of processed_data to ensure alignment\n",
    "processed_data.reset_index(drop=True, inplace=True)\n",
    "y_encoded = pd.Series(y_encoded) \n",
    "\n",
    "# Assuming 'cellularity' should map directly to the target\n",
    "if 'cellularity' in processed_data.columns:\n",
    "    # Ensure no NaN values or unexpected categories before encoding\n",
    "    print(\"Unique values in cellularity before encoding:\", processed_data['cellularity'].unique())\n",
    "\n",
    "    # Directly using 'cellularity' as the target if not already doing so\n",
    "    y = processed_data['cellularity'].copy()\n",
    "\n",
    "    # Using LabelEncoder to encode the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Ensure the encoder is fitting expected classes\n",
    "    print(\"Classes found by LabelEncoder:\", label_encoder.classes_)\n",
    "\n",
    "# Filter data\n",
    "filtered_data = processed_data[processed_data['cellularity'].isin([1, 2, 3])]\n",
    "filtered_y_encoded = y_encoded[filtered_data.index]\n",
    "\n",
    "# Confirming the filtered targets\n",
    "print(\"Unique y values after filtering:\", np.unique(filtered_y_encoded))\n",
    "\n",
    "# Proceed with train-test split and resampling\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    filtered_data, filtered_y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "ros_multi = RandomOverSampler(random_state=42)\n",
    "X_train_multi_ros, y_train_multi_ros = ros_multi.fit_resample(X_train_multi, y_train_multi)\n",
    "\n",
    "# Recheck the unique values in y_train_multi_ros\n",
    "print(\"Unique y values in training data after resample:\", np.unique(y_train_multi_ros))\n",
    "\n",
    "# Adjusting the scale_pos_weight parameter for XGBoost based on class distribution\n",
    "scale_pos_weight_multi = {}\n",
    "for class_label in np.unique(y_train_multi_ros):\n",
    "    scale_pos_weight_multi[class_label] = sum(y_train_multi_ros == class_label) / sum(y_train_multi_ros != class_label)\n",
    "\n",
    "# Convert object columns to category type\n",
    "object_columns = X_train_multi_ros.select_dtypes(include=['object']).columns\n",
    "if not object_columns.empty:\n",
    "    X_train_multi_ros[object_columns] = X_train_multi_ros[object_columns].astype('category')\n",
    "    X_test_multi[object_columns] = X_test_multi[object_columns].astype('category')\n",
    "\n",
    "# Create DMatrix for train and test sets, ensuring no conversion issues\n",
    "dtrain_multi = xgb.DMatrix(X_train_multi_ros, label=y_train_multi_ros, enable_categorical=True)\n",
    "dtest_multi = xgb.DMatrix(X_test_multi, label=y_test_multi, enable_categorical=True)\n",
    "\n",
    "print(np.unique(y_train_multi_ros))\n",
    "\n",
    "# Specify parameters\n",
    "params_multi = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'multi:softmax',\n",
    "    'device':'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 15,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1.5,\n",
    "    'alpha': 0.01,\n",
    "    'num_parallel_tree':5\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "multi_class_model = xgb.train(params_multi, dtrain_multi, num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_multi = multi_class_model.predict(dtest_multi)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Multi-Class Model Accuracy:\", accuracy_score(y_test_multi, y_pred_multi))\n",
    "print(\"Multi-Class Model Confusion Matrix:\\n\", confusion_matrix(y_test_multi, y_pred_multi))\n",
    "print(\"Multi-Class Model Classification Report:\\n\", classification_report(y_test_multi, y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import TrainingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 - Data Preprocessing \n",
    "# Load the data\n",
    "data = pd.read_csv('/content/data.csv', low_memory=False)\n",
    "\n",
    "# Function to drop columns that have the same value across all rows\n",
    "def remove_infrequent_categories(data, threshold=0.05):\n",
    "    filtered_data = data.copy()\n",
    "    categorical_columns = filtered_data.select_dtypes(include='object').columns\n",
    "    categorical_columns = [column for column in categorical_columns if column.endswith('_mut')]\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        filtered_data = filtered_data.loc[filtered_data[column].isin(filtered_data[column].value_counts().index[filtered_data[column].value_counts()/len(filtered_data) > threshold])]\n",
    "    return filtered_data\n",
    "\n",
    "def drop_single_class_columns(df):\n",
    "    unique_value_counts = df.nunique()\n",
    "    single_value_columns = unique_value_counts[unique_value_counts == 1].index\n",
    "    return df.drop(columns=single_value_columns)\n",
    "\n",
    "# Function to one-hot encode specified categorical columns\n",
    "def one_hot_encode_columns(df, columns, encoder, isTrain):\n",
    "    if isTrain:\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        encoded_data = encoder.fit_transform(df[columns])\n",
    "    else:\n",
    "        encoded_data = encoder.transform(df[columns])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns))\n",
    "    return df.drop(columns, axis=1).join(encoded_df), encoder\n",
    "\n",
    "# Main preprocessing function\n",
    "def data_preprocess(df, encoder=None, isTrain=True):\n",
    "    df = drop_single_class_columns(df)\n",
    "\n",
    "    # Fill missing values for numerical columns\n",
    "    numerical_columns = ['neoplasm_histologic_grade', 'mutation_count', 'tumor_size', 'tumor_stage']\n",
    "    df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "\n",
    "    # Identify and fill missing values for all other numerical columns just in case\n",
    "    other_numerical_columns = df.select_dtypes(include=[np.number]).columns.difference(numerical_columns)\n",
    "    df[other_numerical_columns] = df[other_numerical_columns].fillna(df[other_numerical_columns].median())\n",
    "\n",
    "    # Handle missing values and encode categorical variables\n",
    "    categorical_columns = ['pr_status', 'pam50_+_claudin-low_subtype', 'primary_tumor_laterality', 'inferred_menopausal_state', 'her2_status', 'er_status', 'er_status_measured_by_ihc', '3-gene_classifier_subtype', 'death_from_cancer']\n",
    "    for column in categorical_columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    # Ensure all other categorical columns are also filled with the most frequent value\n",
    "    other_categorical_columns = df.select_dtypes(include=['object', 'category']).columns.difference(categorical_columns)\n",
    "    for column in other_categorical_columns:\n",
    "        df[column] = df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    if isTrain:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_data = encoder.fit_transform(df[categorical_columns])\n",
    "    else:\n",
    "        encoded_data = encoder.transform(df[categorical_columns])\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "    df = df.drop(categorical_columns, axis=1).join(encoded_df)\n",
    "\n",
    "    # Handling 'cellularity' with predefined mapping and filling NaNs\n",
    "    if 'cellularity' in df.columns:\n",
    "        df['cellularity'] = df['cellularity'].str.strip()  # Strip whitespace\n",
    "        mapping = {\n",
    "            'Low': 1,\n",
    "            'Moderate': 2,\n",
    "            'High': 3,\n",
    "        }\n",
    "        df['cellularity'] = df['cellularity'].map(mapping).fillna(2)  # Filling NaNs with 'Moderate' assumed as 2\n",
    "\n",
    "    # Handling 'her2_status_measured_by_snp6' with predefined mapping and dropping rows with 'UNDEF'\n",
    "    if 'her2_status_measured_by_snp6' in df.columns:\n",
    "        df = df[df['her2_status_measured_by_snp6'] != 'UNDEF']\n",
    "        df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].str.strip()\n",
    "        her2_mapping = {\n",
    "            'LOSS': -1,\n",
    "            'NEUTRAL': 0,\n",
    "            'GAIN': 1,\n",
    "        }\n",
    "        df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].map(her2_mapping)\n",
    "\n",
    "    # Label encoding for the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = df.pop(\"cancer_type\")\n",
    "    y_encoded = label_encoder.fit_transform(y) if isTrain else label_encoder.transform(y)\n",
    "    y_binary = (y_encoded == 0).astype(int)\n",
    "\n",
    "    return df, y_encoded, y_binary, label_encoder, encoder\n",
    "\n",
    "# Preprocess the data\n",
    "X, y, y_binary, label_encoder, encoder = data_preprocess(data.copy(), isTrain=True)\n",
    "\n",
    "# Split data into train, validate, test sets\n",
    "X_train, X_temp, y_train, y_temp, y_train_binary, y_temp_binary = train_test_split(X, y, y_binary, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 - Train binary classification model\n",
    "# Split the training data for binary classification\n",
    "X_train_binary, _, y_train_binary, _ = train_test_split(X_train, y_train_binary, test_size=0.2, random_state=42)\n",
    "X_test_binary, X_val_binary, y_test_binary, y_val_binary = train_test_split(X_temp, y_temp_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "# Apply Random Over Sampler to the training data\n",
    "ros = RandomOverSampler(random_state=23)\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train_binary, y_train_binary)\n",
    "\n",
    "# Adjusting the scale_pos_weight parameter for XGBoost based on class distribution\n",
    "scale_pos_weight = sum(y_train_ros == 0) / sum(y_train_ros == 1)\n",
    "\n",
    "# Convert all columns of type 'object' to 'category'\n",
    "categorical_columns = X_train_ros.select_dtypes(include=['object']).columns\n",
    "X_train_ros[categorical_columns] = X_train_ros[categorical_columns].astype('category')\n",
    "X_test_binary[categorical_columns] = X_test_binary[categorical_columns].astype('category')\n",
    "X_val_binary[categorical_columns] = X_val_binary[categorical_columns].astype('category')\n",
    "\n",
    "# Create DMatrix for train and test sets\n",
    "dtrain_binary = xgb.DMatrix(X_train_ros, label=y_train_ros, enable_categorical=True)\n",
    "dtest_binary = xgb.DMatrix(X_test_binary, label=y_test_binary, enable_categorical=True)\n",
    "dval_binary = xgb.DMatrix(X_val_binary, label=y_val_binary, enable_categorical=True)\n",
    "\n",
    "# Specify parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'rmse',\n",
    "    'num_class': 1,\n",
    "    'device':'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1,\n",
    "    'alpha': 0.01,\n",
    "    'scale_pos_weight': scale_pos_weight,  # Applying class weight\n",
    "    'num_parallel_tree':5,\n",
    "}\n",
    "\n",
    "# Define the evaluation log storage callback, inheriting from TrainingCallback\n",
    "class EvaluationHistoryCallback(TrainingCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.evaluation_results = {}\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if not evals_log:\n",
    "            return False\n",
    "        for dataset_name, metric_dict in evals_log.items():\n",
    "            for metric_name, log in metric_dict.items():\n",
    "                full_metric_name = f\"{dataset_name}-{metric_name}\"\n",
    "                if full_metric_name not in self.evaluation_results:\n",
    "                    self.evaluation_results[full_metric_name] = []\n",
    "                self.evaluation_results[full_metric_name].append(log[-1])\n",
    "        return False  # Return False to continue training\n",
    "\n",
    "# Initialize callback\n",
    "eval_history = EvaluationHistoryCallback()\n",
    "\n",
    "# Train the model with evaluation set and custom callback\n",
    "eval_set = [(dtrain_binary, 'train'), (dval_binary, 'eval')]\n",
    "binary_model = xgb.train(params, dtrain_binary, num_boost_round=100, evals=eval_set, early_stopping_rounds=10, verbose_eval=True, callbacks=[eval_history])\n",
    "\n",
    "# Access the stored evaluation results\n",
    "results = eval_history.evaluation_results\n",
    "train_rmse = results.get('train-rmse', [])\n",
    "eval_rmse = results.get('eval-rmse', [])\n",
    "epochs = len(train_rmse)\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# Plot learning curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, train_rmse, label='Train RMSE')\n",
    "ax.plot(x_axis, eval_rmse, label='Validation RMSE')\n",
    "ax.legend()\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Learning Curve for Binary Model')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_binary = binary_model.predict(dtest_binary)\n",
    "y_pred_binary = np.round(y_pred_binary)  # Convert probabilities to binary output\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Binary Model Accuracy:\", accuracy_score(y_test_binary, y_pred_binary))\n",
    "print(\"Binary Model Confusion Matrix:\\n\", confusion_matrix(y_test_binary, y_pred_binary))\n",
    "print(\"Binary Model Classification Report:\\n\", classification_report(y_test_binary, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 - Prepare and train multi-class model\n",
    "# Apply preprocessing and ensure the target reflects 'cellularity' correctly\n",
    "processed_data, y_encoded, y_binary, label_encoder, encoder = data_preprocess(data.copy(), isTrain=True)\n",
    "\n",
    "# Reset indices of processed_data to ensure alignment\n",
    "processed_data.reset_index(drop=True, inplace=True)\n",
    "y_encoded = pd.Series(y_encoded) \n",
    "\n",
    "# Assuming 'cellularity' should map directly to the target\n",
    "if 'cellularity' in processed_data.columns:\n",
    "    # Ensure no NaN values or unexpected categories before encoding\n",
    "    print(\"Unique values in cellularity before encoding:\", processed_data['cellularity'].unique())\n",
    "\n",
    "    # Directly using 'cellularity' as the target if not already doing so\n",
    "    y = processed_data['cellularity'].copy()\n",
    "\n",
    "    # Using LabelEncoder to encode the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Ensure the encoder is fitting expected classes\n",
    "    print(\"Classes found by LabelEncoder:\", label_encoder.classes_)\n",
    "\n",
    "# Filter data\n",
    "filtered_data = processed_data[processed_data['cellularity'].isin([1, 2, 3])]\n",
    "filtered_y_encoded = y_encoded[filtered_data.index]\n",
    "\n",
    "# Confirming the filtered targets\n",
    "print(\"Unique y values after filtering:\", np.unique(filtered_y_encoded))\n",
    "\n",
    "# Proceed with train-test split and resampling\n",
    "X_train_multi, X_temp_multi, y_train_multi, y_temp_multi = train_test_split(filtered_data, filtered_y_encoded, test_size=0.2, random_state=42)\n",
    "X_test_multi, X_val_multi, y_test_multi, y_val_multi = train_test_split(X_temp_multi, y_temp_multi, test_size=0.5, random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "ros_multi = RandomOverSampler(random_state=42)\n",
    "X_train_multi_ros, y_train_multi_ros = ros_multi.fit_resample(X_train_multi, y_train_multi)\n",
    "\n",
    "# Recheck the unique values in y_train_multi_ros\n",
    "print(\"Unique y values in training data after resample:\", np.unique(y_train_multi_ros))\n",
    "\n",
    "# Adjusting the scale_pos_weight parameter for XGBoost based on class distribution\n",
    "scale_pos_weight_multi = {}\n",
    "for class_label in np.unique(y_train_multi_ros):\n",
    "    scale_pos_weight_multi[class_label] = sum(y_train_multi_ros == class_label) / sum(y_train_multi_ros != class_label)\n",
    "\n",
    "# Convert object columns to category type\n",
    "object_columns = X_train_multi_ros.select_dtypes(include=['object']).columns\n",
    "if not object_columns.empty:\n",
    "    X_train_multi_ros[object_columns] = X_train_multi_ros[object_columns].astype('category')\n",
    "    X_test_multi[object_columns] = X_test_multi[object_columns].astype('category')\n",
    "    X_val_multi[object_columns] = X_val_multi[object_columns].astype('category')\n",
    "\n",
    "print(np.unique(y_train_multi_ros))\n",
    "\n",
    "# Specify parameters\n",
    "params_multi = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'mlogloss',  \n",
    "    'device': 'cuda',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 15,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1.5,\n",
    "    'alpha': 0.01,\n",
    "    'num_parallel_tree': 5\n",
    "}\n",
    "\n",
    "# Prepare DMatrix for training and validation\n",
    "dtrain_multi = xgb.DMatrix(X_train_multi_ros, label=y_train_multi_ros, enable_categorical=True)\n",
    "dval_multi = xgb.DMatrix(X_val_multi, label=y_val_multi, enable_categorical=True)\n",
    "dtest_multi = xgb.DMatrix(X_test_multi, label=y_test_multi, enable_categorical=True)\n",
    "\n",
    "# Check the sizes right before training\n",
    "print(\"Training size:\", dtrain_multi.num_row(), \"Labels:\", len(y_train_multi_ros))\n",
    "print(\"Validation size:\", dval_multi.num_row(), \"Labels:\", len(y_val_multi))\n",
    "\n",
    "# Initialize callback\n",
    "eval_history_multi = EvaluationHistoryCallback()\n",
    "\n",
    "# Train the model with evaluation set and custom callback\n",
    "eval_set_multi = [(dtrain_multi, 'train'), (dval_multi, 'eval')]\n",
    "multi_class_model = xgb.train(params_multi, dtrain_multi, num_boost_round=100, evals=eval_set_multi, early_stopping_rounds=10, verbose_eval=True, callbacks=[eval_history_multi])\n",
    "\n",
    "# Access the stored evaluation results\n",
    "results_multi = eval_history_multi.evaluation_results\n",
    "train_mlogloss = results_multi.get('train-mlogloss', [])\n",
    "eval_mlogloss = results_multi.get('eval-mlogloss', [])\n",
    "epochs_multi = len(train_mlogloss)\n",
    "x_axis_multi = range(0, epochs_multi)\n",
    "\n",
    "# Plot learning curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis_multi, train_mlogloss, label='Train MLogLoss')\n",
    "ax.plot(x_axis_multi, eval_mlogloss, label='Validation MLogLoss')\n",
    "ax.legend()\n",
    "plt.ylabel('MLogLoss')\n",
    "plt.title('Learning Curve for Multi-Class Model')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_multi = multi_class_model.predict(dtest_multi)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Multi-Class Model Accuracy:\", accuracy_score(y_test_multi, y_pred_multi))\n",
    "print(\"Multi-Class Model Confusion Matrix:\\n\", confusion_matrix(y_test_multi, y_pred_multi))\n",
    "print(\"Multi-Class Model Classification Report:\\n\", classification_report(y_test_multi, y_pred_multi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 - Tuning the first Model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Possible ranges of hyperparameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Function to perform manual cross-validation\n",
    "def manual_cv(params, n_splits=3):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X_train_ros):\n",
    "        X_train_kf, X_test_kf = X_train_ros.iloc[train_index], X_train_ros.iloc[test_index]\n",
    "        y_train_kf, y_test_kf = y_train_ros[train_index], y_train_ros[test_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_kf, label=y_train_kf, enable_categorical=True)\n",
    "        dtest = xgb.DMatrix(X_test_kf, label=y_test_kf, enable_categorical=True)\n",
    "\n",
    "        model = xgb.train(params, dtrain, num_boost_round=params['num_boost_round'])\n",
    "        y_pred = model.predict(dtest)\n",
    "        y_pred_binary = np.round(y_pred)\n",
    "        auc = roc_auc_score(y_test_kf, y_pred_binary)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Iterate over combinations of parameters\n",
    "from itertools import product\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "for combination in product(*param_grid.values()):\n",
    "    params = dict(zip(param_grid.keys(), combination))\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params['scale_pos_weight'] = scale_pos_weight  # Keep the scale_pos_weight from your original setup\n",
    "    params['tree_method'] = 'gpu_hist'  # Ensure to use GPU if available\n",
    "\n",
    "    current_score = manual_cv(params)\n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "        best_params = params\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "final_dtrain = xgb.DMatrix(X_train_ros, label=y_train_ros, enable_categorical=True)\n",
    "final_model = xgb.train(best_params, final_dtrain, num_boost_round=best_params['num_boost_round'])\n",
    "\n",
    "# Evaluate the final model\n",
    "final_dtest = xgb.DMatrix(X_test_binary, label=y_test_binary)\n",
    "y_pred_final = final_model.predict(final_dtest)\n",
    "y_pred_final = np.round(y_pred_final)  # Convert probabilities to binary output\n",
    "\n",
    "print(\"Final Model Accuracy:\", accuracy_score(y_test_binary, y_pred_final))\n",
    "print(\"Final Model Confusion Matrix:\\n\", confusion_matrix(y_test_binary, y_pred_final))\n",
    "print(\"Final Model Classification Report:\\n\", classification_report(y_test_binary, y_pred_final))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
