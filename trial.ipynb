{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(df):\n",
    "    y = df[\"cancer_type\"]\n",
    "    label_encoder = LabelEncoder();\n",
    "    y  = label_encoder.fit_transform(y)\n",
    "    y = pd.Series(y)\n",
    "    df = df.drop('cancer_type', axis = 1)\n",
    "\n",
    "    # label encoding for cellularity 40 nan values transformed to 0\n",
    "    mapping = {\n",
    "        'Low': 1,\n",
    "        'Moderate': 2,\n",
    "        'High': 3,\n",
    "    }\n",
    "    df['cellularity'] = df['cellularity'].str.strip()\n",
    "    df[\"cellularity\"] = df[\"cellularity\"].map(mapping)\n",
    "    df[\"cellularity\"] = df[\"cellularity\"].fillna(0)\n",
    "\n",
    "\n",
    "    # dropping patient_id (irrelevant info)\n",
    "    df = df.drop('patient_id', axis=1)\n",
    "\n",
    "    #label encoding pam50_+_claudin-low_subtype\n",
    "    df['pam50_+_claudin-low_subtype'] =label_encoder.fit_transform( df['pam50_+_claudin-low_subtype'])\n",
    "\n",
    "    df['er_status'] =label_encoder.fit_transform( df['er_status'])\n",
    "\n",
    "    df['er_status_measured_by_ihc'] = label_encoder.fit_transform(df['er_status_measured_by_ihc'])\n",
    "\n",
    "    df['her2_status'] = label_encoder.fit_transform(df['her2_status'])\n",
    "\n",
    "    her2_mapping={\n",
    "    'LOSS' : 0,\n",
    "    'NEUTRAL' : 1,\n",
    "    'GAIN' : 3,\n",
    "    'UNDEF' : 1\n",
    "    }\n",
    "\n",
    "    df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].str.strip()\n",
    "    df['her2_status_measured_by_snp6'] = df['her2_status_measured_by_snp6'].map(her2_mapping)\n",
    "\n",
    "    df['inferred_menopausal_state'] = label_encoder.fit_transform(df['inferred_menopausal_state'])\n",
    "\n",
    "    map_laterality = {\n",
    "    'Right':1,\n",
    "    'Left':-1,\n",
    "    }\n",
    "    df['primary_tumor_laterality'] = df['primary_tumor_laterality'].str.strip()\n",
    "    df['primary_tumor_laterality'] = df['primary_tumor_laterality'].map(map_laterality)\n",
    "    df['primary_tumor_laterality'] = df['primary_tumor_laterality'].fillna(0)\n",
    "\n",
    "    df['pr_status'] = label_encoder.fit_transform(df['pr_status'])\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['3-gene_classifier_subtype'])\n",
    "\n",
    "    df = pd.get_dummies(df, columns=['death_from_cancer'])\n",
    "\n",
    "    tumor_mean = df['tumor_size'].mean()\n",
    "    df[\"tumor_size\"] = df[\"tumor_size\"].fillna(tumor_mean)\n",
    "\n",
    "    mutation_mean = df['mutation_count'].mean()\n",
    "    df['mutation_count'] = df['mutation_count'].fillna(mutation_mean)\n",
    "\n",
    "    df['neoplasm_histologic_grade'] = df['neoplasm_histologic_grade'].fillna(3)\n",
    "\n",
    "    majority_value = df['tumor_stage'].mode()[0]\n",
    "    df['tumor_stage'].fillna(majority_value, inplace=True)\n",
    "    df['tumor_stage']=label_encoder.fit_transform(df['tumor_stage'])\n",
    "\n",
    "    label_encoders = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            # Create a label encoder for each categorical column\n",
    "            le = LabelEncoder()\n",
    "\n",
    "            # Fit the label encoder and transform the data\n",
    "            df[column] = le.fit_transform(df[column].astype(str))\n",
    "\n",
    "            # Store the label encoder in a dictionary in case you need to reverse the encoding or use it later\n",
    "            label_encoders[column] = le\n",
    "    \n",
    "    last_seven = df.iloc[:, -7:]\n",
    "    part_before = df.iloc[:, :2]  # Columns up to the 19th (0-based index, so it includes columns 0-18)\n",
    "    part_after = df.iloc[:, 2:]\n",
    "    df = pd.concat([part_before, last_seven, part_after], axis=1)\n",
    "    df = df.iloc[:, :-7]\n",
    "\n",
    "\n",
    "    \n",
    "    return df,y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32912\\4001528013.py:1: DtypeWarning: Columns (662,664,676,677,683,685,686,687) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data.csv')\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32912\\3463248367.py:66: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['tumor_stage'].fillna(majority_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "X, y = data_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_classification(X_train, y_train, weights, multiclass=False):\n",
    "    # Logistic regression\n",
    "    param_grid_logistic = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    log_r = LogisticRegression(random_state=42, max_iter=1000, class_weight=weights)\n",
    "    if multiclass == True :\n",
    "        log_r = LogisticRegression(multi_class='multinomial', random_state=42, max_iter=1000, class_weight=weights)\n",
    "    grid_search_logistic = GridSearchCV(\n",
    "        log_r, \n",
    "        param_grid= param_grid_logistic, \n",
    "        cv=5, \n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid_search_logistic.fit(X_train, y_train)\n",
    "    best_model_logistic = grid_search_logistic.best_estimator_\n",
    "\n",
    "    # Decision tree\n",
    "    param_grid_dt = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': range(1,20),\n",
    "        'min_samples_split': range(2,21),\n",
    "        'min_samples_leaf': range(1,21)\n",
    "    }\n",
    "    grid_search_dt = GridSearchCV(\n",
    "        DecisionTreeClassifier(random_state=42, class_weight=weights),\n",
    "        param_grid= param_grid_dt,\n",
    "        cv = 5,\n",
    "        scoring = 'accuracy',\n",
    "        n_jobs= -1\n",
    "    )\n",
    "    grid_search_dt.fit(X_train, y_train) \n",
    "    best_model_dt = grid_search_dt.best_estimator_\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    sample_weights = compute_sample_weight(class_weight=weights, y=y_train)\n",
    "    param_grid_gb = {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 4],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    grid_search_gb = GridSearchCV(\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        param_grid= param_grid_gb,\n",
    "        cv= 5,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    grid_search_gb.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    best_model_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "    #Random forest\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200, 300],  \n",
    "        'max_depth': [None, 10, 20, 30],  \n",
    "        'min_samples_split': [2, 5, 10],  \n",
    "        'min_samples_leaf': [1, 2, 4],    \n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    grid_search_rf = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42, class_weight=weights),\n",
    "        param_grid= param_grid_rf,\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_model_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    return best_model_logistic, best_model_dt, best_model_gb, best_model_rf \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_model_logistic, best_model_dt, best_model_gb, best_model_rf = warmup_classification(X_train, y_train,class_weight_dict, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization_class(logistic = best_model_logistic, decision_tree=best_model_dt, gradient_boost=best_model_gb, random_forest=best_model_rf, X_train=X_train, y_train=y_train, X_val=X_test, y_val=y_test):\n",
    "    predictions_train = logistic.predict(X_train)\n",
    "    predictions_val = logistic.predict(X_val)\n",
    "    logistic_train = accuracy_score(predictions_train,y_train)\n",
    "    logistic_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Logistic Regression\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',logistic_train*100,'%')\n",
    "    print('val: ',logistic_val*100,'%')\n",
    "\n",
    "    predictions_train = decision_tree.predict(X_train)\n",
    "    predictions_val = decision_tree.predict(X_val)\n",
    "    dt_train = accuracy_score(predictions_train,y_train)\n",
    "    dt_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Decision Tree\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',dt_train*100,'%')\n",
    "    print('val: ',dt_val*100,'%')\n",
    "\n",
    "    predictions_train = gradient_boost.predict(X_train)\n",
    "    predictions_val = gradient_boost.predict(X_val)\n",
    "    gb_train = accuracy_score(predictions_train,y_train)\n",
    "    gb_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Gradient Boost\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',gb_train*100,'%')\n",
    "    print('val: ',gb_val*100,'%')\n",
    "\n",
    "    predictions_train = random_forest.predict(X_train)\n",
    "    predictions_val = random_forest.predict(X_val)\n",
    "    rf_train = accuracy_score(predictions_train,y_train)\n",
    "    rf_val = accuracy_score(predictions_val, y_val)\n",
    "    print(\"\\033[1m\" + \"Random Forest\" + \"\\033[0m\"+\":\")\n",
    "    print('train: ',rf_train*100,'%')\n",
    "    print('val: ',rf_val*100,'%')\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogistic Regression\u001b[0m:\n",
      "train:  42.4962852897474 %\n",
      "val:  36.0 %\n",
      "\u001b[1mDecision Tree\u001b[0m:\n",
      "train:  99.10846953937593 %\n",
      "val:  62.0 %\n",
      "\u001b[1mGradient Boost\u001b[0m:\n",
      "train:  100.0 %\n",
      "val:  74.0 %\n",
      "\u001b[1mRandom Forest\u001b[0m:\n",
      "train:  100.0 %\n",
      "val:  78.66666666666666 %\n"
     ]
    }
   ],
   "source": [
    "visualization_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_dist_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "}\n",
    "\n",
    "svm = SVC(random_state=42,decision_function_shape='ovr', class_weight=class_weight_dict)\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    svm, \n",
    "    param_distributions=param_dist_svm, \n",
    "    n_iter=10,  # You can adjust the number of iterations\n",
    "    refit=True, \n",
    "    verbose=3, \n",
    "    cv=5, \n",
    "    n_jobs=-1,\n",
    "    random_state=42  # It's good to set a random_state for reproducibility\n",
    ")\n",
    "random_search_svm.fit(X_train, y_train)\n",
    "svm = random_search_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSVM\u001b[0m:\n",
      "train:  100.0 %\n",
      "val:  78.66666666666666 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_train = svm.predict(X_train)\n",
    "predictions_val = svm.predict(X_test)\n",
    "svm_train = accuracy_score(predictions_train,y_train)\n",
    "svm_val = accuracy_score(predictions_val, y_test)\n",
    "print(\"\\033[1m\" + \"SVM\" + \"\\033[0m\"+\":\")\n",
    "print('train: ',svm_train*100,'%')\n",
    "print('val: ',svm_val*100,'%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
